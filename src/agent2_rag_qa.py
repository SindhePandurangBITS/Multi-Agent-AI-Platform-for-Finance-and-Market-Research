from langchain_community.document_loaders import UnstructuredPDFLoader 
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import PGVector
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import Chroma
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from sqlalchemy import create_engine, text
import os

embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-MiniLM-L6-v2"
)
print(" Embedding model loaded.")
PGVECTOR_CONN_STRING = "postgresql+psycopg2://ai:ai@localhost:5532/ai"
COLLECTION_NAME = "goldman_ai_bfsi_2024"
LOCAL_PDF = "goldman_ai_bfsi.pdf"
PDF_URL = "https://www.ewadirect.com/proceedings/aemps/article/view/23748/pdf"

vectorstore = Chroma(
    collection_name="goldman_ai_bfsi_2024",
    embedding_function=embedding_model,
    persist_directory="./chroma_db"
)
print("Vector database ready.")


def load_and_index_pdf(pdf_url, vectorstore, collection_name, local_pdf):
    if not os.path.exists(local_pdf):
        import requests
        print("Downloading PDF...")
        r = requests.get(pdf_url)
        with open(local_pdf, "wb") as f:
            f.write(r.content)
    
    print("Loading and chunking PDF...")
    loader = PyPDFLoader(local_pdf)
    docs = loader.load()
    
    # Split documents into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_documents(docs)
    
    # For Chroma, check if documents already exist
    try:
        existing_count = vectorstore._collection.count()
        if existing_count > 0:
            print(f"Collection has {existing_count} documents, skipping indexing.")
        else:
            print("Indexing PDF into vector database...")
            vectorstore.add_documents(chunks)
            print(f"Indexed {len(chunks)} chunks.")
    except:
        print("Indexing PDF into vector database...")
        vectorstore.add_documents(chunks)
        print(f"Indexed {len(chunks)} chunks.")

load_and_index_pdf(PDF_URL, vectorstore, COLLECTION_NAME, LOCAL_PDF)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 8})
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
memory = ConversationBufferMemory(
    memory_key="chat_history", 
    return_messages=True,
    output_key="answer"
)

qa_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Use ONLY the following context from the company report to answer the user's question.

=== CONTEXT ===
{context}

=== QUESTION ===
{question}

Provide a detailed, factual response, well-cited, using bullet points or executive summaries if requested. If asked, use the following report template for your answer:

# Headline

### Executive Overview
{{Succinct summary of main findings and their importance}}

### Background & Context
{{Historical background and market landscape}}

### Core Insights
{{Key discoveries, expert quotes, supporting statistics}}

### Impact Assessment
{{Effects on industry and stakeholders}}

### Future Outlook
{{Trends, forecasts, risks, opportunities}}

### Expert Commentary
{{Additional insights, contrasting opinions}}

### References & Methodology
{{List of cited sources and brief research approach}}

â€” Report generated by Financial Research GPT
Date: {{current_date}} | Time: {{current_time}}
"""
)
agent2 = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    verbose=True,
    chain_type_kwargs={"prompt": qa_prompt}
)
